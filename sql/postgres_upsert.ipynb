{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import exc, create_engine\n",
    "\n",
    "# CREATE INDEX index_name\n",
    "# ON table_name (column1_name, column2_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_connection(rds_schema: str):\n",
    "    \"\"\"\n",
    "    SQL Connection function connecting to my postgres db with schema = nba_source where initial data in ELT lands.\n",
    "    Args:\n",
    "        rds_schema (str): The Schema in the DB to connect to.\n",
    "    Returns:\n",
    "        SQL Connection variable to a specified schema in my PostgreSQL DB\n",
    "    \"\"\"\n",
    "    RDS_USER = os.environ.get(\"RDS_USER\")\n",
    "    RDS_PW = os.environ.get(\"RDS_PW\")\n",
    "    RDS_IP = os.environ.get(\"IP\")\n",
    "    RDS_DB = os.environ.get(\"RDS_DB\")\n",
    "    try:\n",
    "        connection = create_engine(\n",
    "            f\"postgresql+psycopg2://{RDS_USER}:{RDS_PW}@{RDS_IP}:5432/{RDS_DB}\",\n",
    "            connect_args={\"options\": f\"-csearch_path={rds_schema}\"},\n",
    "            # defining schema to connect to\n",
    "            echo=False,\n",
    "        )\n",
    "        print(f\"SQL Connection to schema: {rds_schema} Successful\")\n",
    "        return connection\n",
    "    except exc.SQLAlchemyError as e:\n",
    "        return e\n",
    "\n",
    "def to_sql_upsert(conn, table_name: str, df: pd.DataFrame, table_type: str):\n",
    "    \"\"\"\n",
    "    SQL Table function to upsert a Pandas DataFrame into a SQL Table.\n",
    "\n",
    "    Will create a new table if it doesn't exist.  If it does, it will insert new records and upsert new column values onto existing records (if applicable).\n",
    "\n",
    "    You have to do some extra index stuff to the pandas df to specify what the primary key of the records is (this data does not get upserted).\n",
    "\n",
    "    Args:\n",
    "        conn (SQL Connection): The connection to the SQL DB.\n",
    "\n",
    "        table_name (str): The Table name to write to SQL as.\n",
    "\n",
    "        df (DataFrame): The Pandas DataFrame to store in SQL\n",
    "\n",
    "        table_type (str): A placeholder which should always be \"upsert\"\n",
    "\n",
    "    Returns:\n",
    "        Writes the Pandas DataFrame to a Table in Snowflake in the {nba_source} Schema we connected to.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If the table does not exist, we should just use to_sql to create it - watchout for table_schema\n",
    "        if not conn.execute(\n",
    "            f\"\"\"SELECT EXISTS (\n",
    "                SELECT FROM information_schema.tables \n",
    "                WHERE  table_schema = 'nba_source_dev' \n",
    "                AND    table_name   = '{table_name}');\n",
    "                \"\"\"\n",
    "        ).first()[0]:\n",
    "            df.to_sql(table_name, conn)\n",
    "            return True\n",
    "\n",
    "        # If it already exists...\n",
    "        temp_table_name = f\"temp_{uuid.uuid4().hex[:6]}\"\n",
    "        df.to_sql(temp_table_name, conn, index=True)\n",
    "\n",
    "        index = list(df.index.names)\n",
    "        index_sql_txt = \", \".join([f'\"{i}\"' for i in index])\n",
    "        columns = list(df.columns)\n",
    "        headers = index + columns\n",
    "        headers_sql_txt = \", \".join(\n",
    "            [f'\"{i}\"' for i in headers]\n",
    "        )  # index1, index2, ..., column 1, col2, ...\n",
    "        # col1 = exluded.col1, col2=excluded.col2\n",
    "        # this is excluding the primary key columns needed to identify the unique rows.\n",
    "        update_column_stmt = \", \".join([f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns])\n",
    "\n",
    "        # For the ON CONFLICT clause, postgres requires that the columns have unique constraint\n",
    "        query_pk = f\"\"\"\n",
    "        ALTER TABLE \"{table_name}\" DROP CONSTRAINT IF EXISTS unique_constraint_for_upsert;\n",
    "        ALTER TABLE \"{table_name}\" ADD CONSTRAINT unique_constraint_for_upsert UNIQUE ({index_sql_txt});\n",
    "        \"\"\"\n",
    "\n",
    "        conn.execute(query_pk)\n",
    "\n",
    "        # Compose and execute upsert query\n",
    "        query_upsert = f\"\"\"\n",
    "        INSERT INTO \"{table_name}\" ({headers_sql_txt}) \n",
    "        SELECT {headers_sql_txt} FROM \"{temp_table_name}\"\n",
    "        ON CONFLICT ({index_sql_txt}) DO UPDATE \n",
    "        SET {update_column_stmt};\n",
    "        \"\"\"\n",
    "        conn.execute(query_upsert)\n",
    "        conn.execute(f\"DROP TABLE {temp_table_name}\")\n",
    "        print(f\"SQL Upsert Function Successful, {len(df)} records added or upserted into {table_name}\")\n",
    "        pass\n",
    "    except BaseException as error:\n",
    "        print(f\"SQL Upsert Function Failed for {table_name} ({len(df)} rows), {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Function Completed for april, retrieving 129 rows\n",
      "Schedule Function Completed for may, retrieving 40 rows\n",
      "june currently has no data in basketball-reference, stopping the function and returning data for april may\n"
     ]
    }
   ],
   "source": [
    "def schedule_scraper(year: str, month_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Web Scrape Function to scrape Schedule data by iterating through a list of months\n",
    "    Args:\n",
    "        year (str) - The year to scrape\n",
    "        month_list (list) - List of full-month names to scrape\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of Schedule Data to be stored.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        schedule_df = pd.DataFrame()\n",
    "        completed_months = []\n",
    "        for i in month_list:\n",
    "            url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_games-{i}.html\"\n",
    "            html = requests.get(url).content\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            headers = [th.getText() for th in soup.findAll(\"tr\")[0].findAll(\"th\")]\n",
    "            headers[6] = \"boxScoreLink\"\n",
    "            headers[7] = \"isOT\"\n",
    "            headers = headers[1:]\n",
    "\n",
    "            rows = soup.findAll(\"tr\")[1:]\n",
    "            date_info = [\n",
    "                [th.getText() for th in rows[i].findAll(\"th\")] for i in range(len(rows))\n",
    "            ]\n",
    "\n",
    "            game_info = [\n",
    "                [td.getText() for td in rows[i].findAll(\"td\")] for i in range(len(rows))\n",
    "            ]\n",
    "            date_info = [i[0] for i in date_info]\n",
    "\n",
    "            schedule = pd.DataFrame(game_info, columns=headers)\n",
    "            schedule[\"Date\"] = date_info\n",
    "\n",
    "            print(\n",
    "                f\"Schedule Function Completed for {i}, retrieving {len(schedule)} rows\"\n",
    "            )\n",
    "            completed_months.append(i)\n",
    "            schedule_df = schedule_df.append(schedule)\n",
    "\n",
    "        schedule_df = schedule_df[\n",
    "            [\"Start (ET)\", \"Visitor/Neutral\", \"Home/Neutral\", \"Date\"]\n",
    "        ]\n",
    "        schedule_df[\"proper_date\"] = pd.to_datetime(schedule_df[\"Date\"]).dt.date\n",
    "        schedule_df.columns = schedule_df.columns.str.lower()\n",
    "        schedule_df = schedule_df.rename(\n",
    "            columns={\n",
    "                \"start (et)\": \"start_time\",\n",
    "                \"visitor/neutral\": \"away_team\",\n",
    "                \"home/neutral\": \"home_team\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Schedule Function Completed for {' '.join(completed_months)}, retrieving {len(schedule_df)} total rows\"\n",
    "        )\n",
    "        return schedule_df\n",
    "    except IndexError as index_error:\n",
    "        print(\n",
    "            f\"{i} currently has no data in basketball-reference, stopping the function and returning data for {' '.join(completed_months)}\"\n",
    "        )\n",
    "        schedule_df = schedule_df[\n",
    "            [\"Start (ET)\", \"Visitor/Neutral\", \"Home/Neutral\", \"Date\"]\n",
    "        ]\n",
    "        schedule_df[\"proper_date\"] = pd.to_datetime(schedule_df[\"Date\"]).dt.date\n",
    "        schedule_df.columns = schedule_df.columns.str.lower()\n",
    "        schedule_df = schedule_df.rename(\n",
    "            columns={\n",
    "                \"start (et)\": \"start_time\",\n",
    "                \"visitor/neutral\": \"away_team\",\n",
    "                \"home/neutral\": \"home_team\",\n",
    "            }\n",
    "        )\n",
    "        return schedule_df\n",
    "    except BaseException as e:\n",
    "        print(f\"Schedule Scraper Function Failed, {e}\")\n",
    "        df = []\n",
    "        return df\n",
    "\n",
    "schedule_df = schedule_scraper(\"2022\", [\"april\", \"may\", \"june\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection to schema: nba_source_dev Successful\n"
     ]
    }
   ],
   "source": [
    "conn = sql_connection('nba_source_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_schedule1 = pd.read_csv('dummy_schedule1.csv')\n",
    "dummy_schedule2 = pd.read_csv('dummy_schedule2.csv')\n",
    "dummy_schedule1['proper_date'] = pd.to_datetime(dummy_schedule1['proper_date'])\n",
    "dummy_schedule2['proper_date'] = pd.to_datetime(dummy_schedule2['proper_date'])\n",
    "\n",
    "dummy_schedule1 = dummy_schedule1.set_index(['away_team', 'home_team', 'proper_date'])\n",
    "dummy_schedule1 = dummy_schedule1.rename_axis(['away_team', 'home_team', 'proper_date'])\n",
    "\n",
    "dummy_schedule2 = dummy_schedule2.set_index(['away_team', 'home_team', 'proper_date'])\n",
    "dummy_schedule2 = dummy_schedule2.rename_axis(['away_team', 'home_team', 'proper_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenList(['away_team', 'home_team', 'proper_date'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_schedule1.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_sql_upsert(conn, 'nba_schedule', dummy_schedule2, 'append')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51acfa6005ffec5e74e71d844e0daa05d24ac78244a0bb1b7874b497027552e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
