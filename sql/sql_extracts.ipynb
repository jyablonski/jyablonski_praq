{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import awswrangler as wr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import exc, create_engine\n",
    "\n",
    "yesterday = datetime.now() - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection to schema: nba_source Successful\n"
     ]
    }
   ],
   "source": [
    "def write_to_sql(con, table_name: str, df: pd.DataFrame, table_type: str) -> None:\n",
    "    \"\"\"\n",
    "    SQL Table function to write a pandas data frame in aws_dfname_source format\n",
    "    Args:\n",
    "        con (SQL Connection): The connection to the SQL DB.\n",
    "        table_name (str): The Table name to write to SQL as.\n",
    "        df (DataFrame): The Pandas DataFrame to store in SQL\n",
    "        table_type (str): Whether the table should replace or append to an existing SQL Table under that name\n",
    "    Returns:\n",
    "        Writes the Pandas DataFrame to a Table in Snowflake in the {nba_source} Schema we connected to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df) == 0:\n",
    "            print(f\"{table_name} is empty, not writing to SQL\")\n",
    "        else:\n",
    "            df.to_sql(\n",
    "                con=con,\n",
    "                name=f\"aws_{table_name}_source\",\n",
    "                index=False,\n",
    "                if_exists=table_type,\n",
    "            )\n",
    "            print(\n",
    "                f\"Writing {len(df)} {table_name} rows to aws_{table_name}_source to SQL\"\n",
    "            )\n",
    "    except BaseException as error:\n",
    "        print(f\"SQL Write Script Failed, {error}\")\n",
    "\n",
    "def sql_connection(rds_schema: str):\n",
    "    \"\"\"\n",
    "    SQL Connection function connecting to my postgres db with schema = nba_source where initial data in ELT lands.\n",
    "    Args:\n",
    "        rds_schema (str): The Schema in the DB to connect to.\n",
    "    Returns:\n",
    "        SQL Connection variable to a specified schema in my PostgreSQL DB\n",
    "    \"\"\"\n",
    "    RDS_USER = os.environ.get(\"RDS_USER\")\n",
    "    RDS_PW = os.environ.get(\"RDS_PW\")\n",
    "    RDS_IP = os.environ.get(\"IP\")\n",
    "    RDS_DB = os.environ.get(\"RDS_DB\")\n",
    "    try:\n",
    "        connection = create_engine(\n",
    "            f\"postgresql+psycopg2://{RDS_USER}:{RDS_PW}@{RDS_IP}:5432/{RDS_DB}\",\n",
    "            connect_args={\"options\": f\"-csearch_path={rds_schema}\"},\n",
    "            # defining schema to connect to\n",
    "            echo=False,\n",
    "        )\n",
    "        print(f\"SQL Connection to schema: {rds_schema} Successful\")\n",
    "        return connection\n",
    "    except exc.SQLAlchemyError as e:\n",
    "        return e\n",
    "\n",
    "conn = sql_connection(rds_schema='nba_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxscores = pd.read_sql('select * from aws_boxscores_source', conn)\n",
    "boxscores.to_parquet('sql_transfer/boxscores.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_adv_stats_source', conn)\n",
    "df.to_parquet('sql_transfer/adv_stats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_injury_data_source', conn)\n",
    "df.to_parquet('sql_transfer/injury_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_odds_source', conn)\n",
    "df.to_parquet('sql_transfer/odds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_pbp_data_source', conn)\n",
    "df.to_parquet('sql_transfer/pbp_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_preseason_odds_source', conn)\n",
    "df.to_parquet('sql_transfer/preseason_odds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_reddit_comment_data_source', conn)\n",
    "df.to_parquet('sql_transfer/reddit_comments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_reddit_data_source', conn)\n",
    "df.to_parquet('sql_transfer/reddit_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_schedule_source', conn)\n",
    "df.to_parquet('sql_transfer/schedule.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_shooting_stats_source', conn)\n",
    "df.to_parquet('sql_transfer/shooting_stats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_stats_source', conn)\n",
    "df.to_parquet('sql_transfer/stats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_transactions_source', conn)\n",
    "df.to_parquet('sql_transfer/transactions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_twitter_data_source', conn)\n",
    "df.to_parquet('sql_transfer/twitter_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from aws_twitter_tweepy_data_source', conn)\n",
    "df.to_parquet('sql_transfer/twitter_tweepy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from staging_seed_player_attributes', conn)\n",
    "df.to_parquet('sql_transfer/player_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from staging_seed_team_attributes', conn)\n",
    "df.to_parquet('sql_transfer/team_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python_aws-McJt4gWW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c6a89d9b91bda76c14e46d4b77530c453739b0f65fa8daf80e192463b63a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
