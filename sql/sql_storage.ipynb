{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jacob/Documents/python_aws'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import awswrangler as wr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import exc, create_engine\n",
    "\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection to schema: nba_source Successful\n"
     ]
    }
   ],
   "source": [
    "def write_to_sql(con, table_name: str, df: pd.DataFrame, table_type: str) -> None:\n",
    "    \"\"\"\n",
    "    SQL Table function to write a pandas data frame in aws_dfname_source format\n",
    "    Args:\n",
    "        con (SQL Connection): The connection to the SQL DB.\n",
    "        table_name (str): The Table name to write to SQL as.\n",
    "        df (DataFrame): The Pandas DataFrame to store in SQL\n",
    "        table_type (str): Whether the table should replace or append to an existing SQL Table under that name\n",
    "    Returns:\n",
    "        Writes the Pandas DataFrame to a Table in Snowflake in the {nba_source} Schema we connected to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df) == 0:\n",
    "            print(f\"{table_name} is empty, not writing to SQL\")\n",
    "        else:\n",
    "            df.to_sql(\n",
    "                con=con,\n",
    "                name=f\"aws_{table_name}_source\",\n",
    "                index=False,\n",
    "                if_exists=table_type,\n",
    "            )\n",
    "            print(\n",
    "                f\"Writing {len(df)} {table_name} rows to aws_{table_name}_source to SQL\"\n",
    "            )\n",
    "    except BaseException as error:\n",
    "        print(f\"SQL Write Script Failed, {error}\")\n",
    "\n",
    "def sql_connection(rds_schema: str):\n",
    "    \"\"\"\n",
    "    SQL Connection function connecting to my postgres db with schema = nba_source where initial data in ELT lands.\n",
    "    Args:\n",
    "        rds_schema (str): The Schema in the DB to connect to.\n",
    "    Returns:\n",
    "        SQL Connection variable to a specified schema in my PostgreSQL DB\n",
    "    \"\"\"\n",
    "    RDS_USER = os.environ.get(\"RDS_USER\")\n",
    "    RDS_PW = os.environ.get(\"RDS_PW\")\n",
    "    RDS_IP = os.environ.get(\"IP\")\n",
    "    RDS_DB = os.environ.get(\"RDS_DB\")\n",
    "    try:\n",
    "        connection = create_engine(\n",
    "            f\"postgresql+psycopg2://{RDS_USER}:{RDS_PW}@{RDS_IP}:5432/{RDS_DB}\",\n",
    "            connect_args={\"options\": f\"-csearch_path={rds_schema}\"},\n",
    "            # defining schema to connect to\n",
    "            echo=False,\n",
    "        )\n",
    "        print(f\"SQL Connection to schema: {rds_schema} Successful\")\n",
    "        return connection\n",
    "    except exc.SQLAlchemyError as e:\n",
    "        return e\n",
    "\n",
    "conn = sql_connection(rds_schema='nba_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 30219 boxscores rows to aws_boxscores_source to SQL\n"
     ]
    }
   ],
   "source": [
    "boxscores = pd.read_parquet('sql_transfer/boxscores.parquet')\n",
    "write_to_sql(conn, 'boxscores', boxscores, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5640 adv_stats rows to aws_adv_stats_source to SQL\n"
     ]
    }
   ],
   "source": [
    "adv_stats = pd.read_parquet('sql_transfer/adv_stats.parquet')\n",
    "write_to_sql(conn, 'adv_stats', adv_stats, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 515 contracts rows to aws_contracts_source to SQL\n"
     ]
    }
   ],
   "source": [
    "contracts = pd.read_parquet('sql_transfer/contracts.parquet')\n",
    "write_to_sql(conn, 'contracts', contracts, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 59 injury_data rows to aws_injury_data_source to SQL\n"
     ]
    }
   ],
   "source": [
    "injury_data = pd.read_parquet('sql_transfer/injury_data.parquet')\n",
    "write_to_sql(conn, 'injury_data', injury_data, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2787 odds rows to aws_odds_source to SQL\n"
     ]
    }
   ],
   "source": [
    "odds = pd.read_parquet('sql_transfer/odds.parquet')\n",
    "write_to_sql(conn, 'odds', odds, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 30 opp_stats rows to aws_opp_stats_source to SQL\n"
     ]
    }
   ],
   "source": [
    "opp_stats = pd.read_parquet('sql_transfer/opp_stats.parquet').query('scrape_date == scrape_date.max()')\n",
    "\n",
    "write_to_sql(conn, 'opp_stats', opp_stats, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 175996 pbp_data rows to aws_pbp_data_source to SQL\n"
     ]
    }
   ],
   "source": [
    "pbp_data = pd.read_parquet('sql_transfer/pbp_data.parquet')\n",
    "write_to_sql(conn, 'pbp_data', pbp_data, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 595 player_attributes rows to aws_player_attributes_source to SQL\n"
     ]
    }
   ],
   "source": [
    "player_attributes = pd.read_parquet('sql_transfer/player_attributes.parquet')\n",
    "write_to_sql(conn, 'player_attributes', player_attributes, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 30 preseason_odds rows to aws_preseason_odds_source to SQL\n"
     ]
    }
   ],
   "source": [
    "preseason_odds = pd.read_parquet('sql_transfer/preseason_odds.parquet')\n",
    "write_to_sql(conn, 'preseason_odds', preseason_odds, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1548999 reddit_comments rows to aws_reddit_comments_source to SQL\n"
     ]
    }
   ],
   "source": [
    "reddit_comments = pd.read_parquet('sql_transfer/reddit_comments.parquet')\n",
    "write_to_sql(conn, 'reddit_comments', reddit_comments, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 6183 reddit_data rows to aws_reddit_data_source to SQL\n"
     ]
    }
   ],
   "source": [
    "reddit_data = pd.read_parquet('sql_transfer/reddit_data.parquet')\n",
    "write_to_sql(conn, 'reddit_data', reddit_data, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1397 schedule rows to aws_schedule_source to SQL\n"
     ]
    }
   ],
   "source": [
    "schedule = pd.read_parquet('sql_transfer/schedule.parquet').drop_duplicates()\n",
    "write_to_sql(conn, 'schedule', schedule, 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 605 shooting_stats rows to aws_shooting_stats_source to SQL\n"
     ]
    }
   ],
   "source": [
    "shooting_stats = pd.read_parquet('sql_transfer/shooting_stats.parquet').query('scrape_date == scrape_date.max()')\n",
    "write_to_sql(conn, 'shooting_stats', shooting_stats, 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 812 stats rows to aws_stats_source to SQL\n"
     ]
    }
   ],
   "source": [
    "stats = pd.read_parquet('sql_transfer/stats.parquet').query('scrape_date == scrape_date.max()')\n",
    "write_to_sql(conn, 'stats', stats, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 30 team_attributes rows to aws_team_attributes_source to SQL\n"
     ]
    }
   ],
   "source": [
    "team_attributes = pd.read_parquet('sql_transfer/team_attributes.parquet')\n",
    "write_to_sql(conn, 'team_attributes', team_attributes, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1935 transactions rows to aws_transactions_source to SQL\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_parquet('sql_transfer/transactions.parquet')\n",
    "write_to_sql(conn, 'transactions', transactions, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 155980 twitter_tweepy rows to aws_twitter_tweepy_source to SQL\n",
      "Writing 261127 twitter_tweets rows to aws_twitter_tweets_source to SQL\n"
     ]
    }
   ],
   "source": [
    "twitter_tweepy = pd.read_parquet('sql_transfer/twitter_tweepy.parquet')\n",
    "twitter_tweets = pd.read_parquet('sql_transfer/twitter_tweets.parquet')\n",
    "\n",
    "write_to_sql(conn, 'twitter_tweepy', twitter_tweepy, 'append')\n",
    "write_to_sql(conn, 'twitter_tweets', twitter_tweets, 'append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python_aws-McJt4gWW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c6a89d9b91bda76c14e46d4b77530c453739b0f65fa8daf80e192463b63a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
