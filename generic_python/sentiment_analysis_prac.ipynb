{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import exc, create_engine\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# import pyarrow\n",
    "import awswrangler as wr\n",
    "# from urllib.error import URLError, HTTPError\n",
    "from utils import *\n",
    "from praw.models import MoreComments\n",
    "import nltk\n",
    "\n",
    "logging.basicConfig(filename='example.log', level=logging.DEBUG, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "logging.info('Starting Logging Function')\n",
    "\n",
    "today = datetime.now().date()\n",
    "yesterday = today - timedelta(1)\n",
    "day = (datetime.now() - timedelta(1)).day\n",
    "month = (datetime.now() - timedelta(1)).month\n",
    "year = (datetime.now() - timedelta(1)).year\n",
    "season_type = 'Regular Season'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()] # this removes punctutation.\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "words = [w for w in words if w.lower() not in stopwords] # remove stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"\n",
    "For some quick analysis, creating a corpus could be overkill.\n",
    "If all you need is a word list,\n",
    "there are simpler ways to achieve that goal.\"\"\"\n",
    "pprint(nltk.word_tokenize(text), width=79, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_fd = nltk.FreqDist([w.lower() for w in fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "text.concordance(\"america\", lines=5) # collection of words along with their context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_list = text.concordance_list(\"america\", lines=2)\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(\n",
    "    \"\"\"Beautiful is better than ugly.\n",
    "    Explicit is better than implicit.\n",
    "    Simple is better than complex.\"\"\"\n",
    ")\n",
    "text = nltk.Text(words)\n",
    "fd = text.vocab()  # Equivalent to fd = nltk.FreqDist(words)\n",
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations can be made up of two or more words. NLTK provides classes to handle several types of collocations:\n",
    "\n",
    "Bigrams: Frequent two-word combinations\n",
    "Trigrams: Frequent three-word combinations\n",
    "Quadgrams: Frequent four-word combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words) # find trigrams\n",
    "finder.ngram_fd.most_common(2)\n",
    "finder.ngram_fd.tabulate(2) # most popular trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isalpha() just means grab only letters, not punctuation and shit\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()] # getting all the words we want to analyze in list format\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words) # find trigrams\n",
    "finder.ngram_fd.most_common(2)\n",
    "finder.ngram_fd.tabulate(2) # most popular trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment score of neg neutral and pos, adds up to 1.\n",
    "# this is vader lexicon, pretrained, good on social media w/ slang and broken sentences, @ symbols etc.\n",
    "# Valence Aware Dictionary and Sentiment Reasoner. VADER\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.4791}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"this product is really not that bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.3, 'pos': 0.7, 'compound': 0.7906}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"this product good to great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.4019}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"@GAPonsonby Yes they would They've done it before  https//t.co/qISmXisGhJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> True RT @EDPC7: #UKIP the common sense party who speak the truth and the truth hurts #Cameron #Miliband and #clegg \n",
      "\n",
      "#GE2015\n",
      "> False @menilleyble ate menille i need youuuu :((((\n",
      "> False @MSmithsonPB Last 20 polls Mean|Mode|Median ave:\n",
      "GRN  5.0%|5%|5%8%%%\n",
      "> False RT @jreynoldsMP: Missed the start. Did Cameron also bring a note reminding everyone Tories were pledged to match Lab's spending plans pre-f…\n",
      "> False RT @hedkandikid: It's sad that people think Nicola Sturgeon represents Scotland. She doesn't. Well maybe 37% of it. #SNPout\n",
      "> False @Kellipage17 I've only been a fan of 5SOS since early June last year :( And I really hate that… I get so upset over it.\n",
      "Not that my parents\n",
      "> False RT @standuptoUKIP: Farage's speech notes for #AskNigelFarage tonight? #GE2015 http//t.co/C1TldhbJoG\n",
      "> False RT @WalesOnline: Will it come back to haunt him? Miliband rules out a deal with the SNP after May 7\n",
      "http//t.co/bOs668mLNj http//t.co/lVje…\n",
      "> True @GAPonsonby Yes they would They've done it before  https//t.co/qISmXisGhJ\n",
      "> False With Miliband breaking the new @UKlabour vow next week (not to team up with SNP). we really need our independence from these lying bastards!\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.00% correct\n"
     ]
    }
   ],
   "source": [
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "        if review_id in positive_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "        if review_id in negative_review_ids:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection to schema: nba_source Successful\n"
     ]
    }
   ],
   "source": [
    "from utils import sql_connection\n",
    "conn = sql_connection('nba_source')\n",
    "df = pd.read_sql(con = conn, sql = 'SELECT * FROM aws_reddit_comment_data_source;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is basically a pretrained ML model\n",
    "# <3 and punctuation enhances sentiment\n",
    "# words after @ and # dont count bc social media.\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df2['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df2['comment']]\n",
    "df2['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df2['comment']]\n",
    "df2['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df2['comment']]\n",
    "df2['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df2['comment']]\n",
    "df2['sentiment'] = np.where(df2['compound'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_sql(con = conn, name = 'aws_reddit_comment_data_source', if_exists = 'replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'score', 'url', 'scrape_date', 'scrape_ts', 'compound',\n",
       "       'neg', 'neu', 'pos', 'sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_comment_cols = ['comment', 'score', 'url', 'scrape_date', 'scrape_ts', 'compound',\n",
    "       'neg', 'neu', 'pos', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comment_cols = ['comment', 'score', 'url', 'scrape_date', 'scrape_ts', 'compound',\n",
    "       'neg', 'neu', 'pos', 'sentiment']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51acfa6005ffec5e74e71d844e0daa05d24ac78244a0bb1b7874b497027552e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
