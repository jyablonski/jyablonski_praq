{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import zipfile\n",
    "import pandas as pd # pandas has one too\n",
    "from pathlib import Path\n",
    "import awswrangler as wr\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_compressed_df(df, dirPath, fileName):\n",
    "    \"\"\"Save a Pandas dataframe as a zipped .csv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        Input dataframe.\n",
    "    dirPath : str or pathlib.PosixPath\n",
    "        Parent directory of the zipped file.\n",
    "    fileName : str\n",
    "        File name without extension.\n",
    "    \"\"\"\n",
    "\n",
    "    dirPath = Path(dirPath)\n",
    "    path_zip = dirPath / f'{fileName}.csv.zip'\n",
    "    txt = df.to_csv(index=False)\n",
    "    with zipfile.ZipFile(path_zip, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.writestr(f'{fileName}.csv', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('nba_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nba_tweets2.csv', index = None, sep = \",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nba_tweets.csv is 11.7 mb, zips are 3.7 mbs\n",
    "df.to_csv('nba_tweets.zip', compression = 'zip', index = None, sep = \",\", header=True, encoding='utf-8-sig')   # zip archive is collection of compressed files.  every file is uniquely compressed.\n",
    "df.to_csv('nba_tweets.gz', compression = 'gzip', index = None, sep = \",\", header=True, encoding='utf-8-sig')   \n",
    "df.to_csv('nba_tweets.tar.gz', compression = 'gzip', index = None, sep = \",\", header=True, encoding='utf-8-sig') \n",
    "\n",
    "# zip archive is collection of compressed files.  every file is uniquely compressed.\n",
    "# compressed collection of uncompressed files.  must decompress everything to access files within\n",
    "# # tar bundles everything up, gz compresses that tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/jacob/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_zip = pd.read_csv('nba_tweets.zip', compression = 'zip', error_bad_lines=False)\n",
    "df_gzip = pd.read_csv('nba_tweets.gz', compression = 'gzip', error_bad_lines=False)\n",
    "df_tar = pd.read_csv('nba_tweets.tar.gz', compression = 'gzip', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://jacobsbucket97/sample_files/nba_tweets.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df = df,\n",
    "    path = \"s3://jacobsbucket97/sample_files/nba_tweets.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df = df,\n",
    "    path = \"s3://jacobsbucket97/sample_files/nba_tweets.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://jacobsbucket97/sample_files/nba_tweets_snappy2.snappy'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df = df,\n",
    "    path = \"s3://jacobsbucket97/sample_files/nba_tweets_snappy2.snappy\",\n",
    "    compression = \"snappy\"\n",
    ")\n",
    "\n",
    "# wr.s3.to_parquet(\n",
    "#     df = df,\n",
    "#     path = \"s3://jacobsbucket97/sample_files/nba_tweets_zip.gzip.parquet\",\n",
    "#     compression = 'gzip'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = wr.s3.read_parquet(\n",
    "#     path = \"s3://jacobsbucket97/sample_files/nba_tweets_snappy2.snappy\"\n",
    "# )\n",
    "\n",
    "df3 = wr.s3.read_parquet(\n",
    "    path = \"s3://jacobsbucket97/sample_files/nba_tweets_zip.gzip.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://jacobsbucket97/sample_files/nba_tweets_zip2.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df = df,\n",
    "    path = \"s3://jacobsbucket97/sample_files/nba_tweets_zip2.parquet\",\n",
    "    compression = 'gzip'\n",
    ")\n",
    "# 2.1 mb for compressed parquet, 3.0 mb for normal parquet, 11.2 for csv, 3.5mb for zipped csv.\n",
    "# i believe the idea is compressed parquet might take longer to read from, but its the most efficient storage option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in first 1000 rows\n",
    "df = pd.read_csv('nba_tweets.csv', nrows = 10) #skiprows = 998"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51acfa6005ffec5e74e71d844e0daa05d24ac78244a0bb1b7874b497027552e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
