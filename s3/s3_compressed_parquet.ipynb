{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low = 2000, high = 2022, size = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2004, 4, 6, 0, 0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def rand_date():\n",
    "    ex_date = datetime(np.random.randint(low = 2000, high = 2022, size = 1)[0],\n",
    "                np.random.randint(low = 1, high = 12, size = 1)[0],\n",
    "                np.random.randint(low = 1, high = 28, size = 1)[0])\n",
    "    return ex_date\n",
    "\n",
    "rand_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['data1'] = np.random.randint(low = 1, high = 1000, size = 1000000)\n",
    "df['data2'] = np.random.randint(low = 1001, high = 2000, size = 1000000)\n",
    "df['data3'] = np.random.randint(low = 2001, high = 3000, size = 1000000)\n",
    "df['data4'] = np.random.randint(low = -3001, high = 4000, size = 1000000)\n",
    "df['data5'] = np.random.randint(low = -4001, high = 5000, size = 1000000)\n",
    "df['date_ts'] = df.apply(lambda x: rand_date(), axis = 1)\n",
    "df['name'] = df.apply(lambda x: fake.name(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'jacobsbucket97-dev/parquet_test/test_textdate.parquet'\n",
    "file_name_snappy = 'jacobsbucket97-dev/parquet_test/test_textdate_snappy.snappy'\n",
    "file_name_gzip = 'jacobsbucket97-dev/parquet_test/test_textdate_gzip.gzip'\n",
    "\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://jacobsbucket97-dev/parquet_test/test_textdate.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(df=df, path = f's3://{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://jacobsbucket97-dev/parquet_test/test_textdate_gzip.gzip'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## awswrangler uses SNAPPY by default on this function\n",
    "wr.s3.to_parquet(df=df, path = f's3://{file_name}', compression = None)\n",
    "wr.s3.to_parquet(df=df, path = f's3://{file_name_snappy}', compression = 'snappy')\n",
    "wr.s3.to_parquet(df=df, path = f's3://{file_name_gzip}', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "I stored a dataframe of 1,000,000 records with 5, 6, and 7 different columns to test S3 storage scaling.\n",
    "\n",
    "So integer data for the 5 data columns ALL stored at the exact same storage size at roughly ~6.9 MB, regardless of the compression used.\n",
    "\n",
    "Adding 1 additional text column with generic name data dramatically increased the storage size.\n",
    "\n",
    "Adding 1 additional static timestamp column DIDNT CHANGE THE DATA SIZE.  Because it's compressed and the 1,000,000 are all the same value.\n",
    "\n",
    "Adding 1 additional dynamic timestamp column DID change the size but not as much as the text.\n",
    "\n",
    "## GZIP\n",
    "    no_text:         6.9 MB\n",
    "    text:           12.2 MB\n",
    "    text+samedatee: 12.2 MB\n",
    "    text+randdatee: 13.8 MB\n",
    "\n",
    "## Snappy\n",
    "    no_text:         6.9 MB\n",
    "    text:           15.5 MB\n",
    "    text+samedatee: 15.5 MB\n",
    "    text+randdatee: 17.1 MB\n",
    "\n",
    "## No Compression\n",
    "    no_text:         7.0 MB\n",
    "    text:           23.2 MB\n",
    "    text+samedatee: 23.2 MB\n",
    "    text+randdatee: 24.9 MB\n",
    "\n",
    "# Conclusions\n",
    "Integer data is pretty efficient.  Text is expensive.  Dates are super efficient.  You get incredible compression savings for storing the exact same values for certain columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's variability in load times bc of network shit, think they're all the same essentially\n",
    "df_s3 = wr.s3.read_parquet(f's3://{file_name}') # 3.6 seconds\n",
    "df_s3_snappy = wr.s3.read_parquet(f's3://{file_name_snappy}') # 2.8\n",
    "df_s3_gz = wr.s3.read_parquet(f's3://{file_name_gzip}') # 3.9 seconds "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c6a89d9b91bda76c14e46d4b77530c453739b0f65fa8daf80e192463b63a97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python_aws-McJt4gWW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
